<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Hugo Academic CV Theme</title>
    <link>http://localhost:1313/post/</link>
      <atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Hugo Blox Builder (https://hugoblox.com)</generator><language>en-us</language><lastBuildDate>Wed, 19 Jun 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>http://localhost:1313/media/icon_hu68170e94a17a2a43d6dcb45cf0e8e589_3079_512x512_fill_lanczos_center_3.png</url>
      <title>Posts</title>
      <link>http://localhost:1313/post/</link>
    </image>
    
    <item>
      <title>Our paper Luciddreamer is selected as Spotlight at CVPR 2024 and featured on Hugging Face Daily</title>
      <link>http://localhost:1313/post/2024-luciddreamer/</link>
      <pubDate>Wed, 19 Jun 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2024-luciddreamer/</guid>
      <description>&lt;p&gt;We are thrilled to announce that our recent work, &lt;em&gt;LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching&lt;/em&gt;, has been selected as a spotlight paper at the prestigious CVPR 2024 conference. This recognition is a testament to the innovative breakthroughs the paper introduces in the field of generative models.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LucidDreamer&lt;/strong&gt; has been designed to transform textual descriptions directly into high-quality 3D models, overcoming previous challenges that limited the detail and accuracy of generated 3D content. The key to our approach lies in a novel method named &lt;strong&gt;Interval Score Matching (ISM)&lt;/strong&gt;, which ensures the generation of detailed, high-quality 3D models by effectively using pre-trained 2D diffusion models as a strong image prior.&lt;/p&gt;
&lt;h3 id=&#34;highlights-of-our-work&#34;&gt;&lt;strong&gt;Highlights of Our Work:&lt;/strong&gt;&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Innovative Technique&lt;/strong&gt;: Our Interval Score Matching significantly enhances the text-to-3D generation process, allowing for the creation of photorealistic 3D models with unprecedented detail and fidelity.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Efficient and Effective&lt;/strong&gt;: Compared to existing methods, LucidDreamer reduces training costs and complexity while delivering superior results, as evidenced by extensive experiments and qualitative assessments.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Community and Industry Impact&lt;/strong&gt;: Since its feature as a daily paper on Hugging Face, our framework has attracted attention from both the academic community and industry practitioners, opening up new possibilities for creative and commercial applications.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;img src=&#34;interests.png&#34; alt=&#34;Alt text&#34;&gt;&lt;/p&gt;
&lt;p&gt;The code for LucidDreamer is available to the public under the &lt;a href=&#34;https://github.com/EnVision-Research/LucidDreamer&#34;&gt;EnVision-Research GitHub repository&lt;/a&gt;, enabling researchers and developers to build upon our work.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Yixun Liang, My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Xin Yang, My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Jiantao Lin, My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Haodong Li, My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Xiaogang Xu, Collaborator from Zhejiang University&lt;/li&gt;
&lt;li&gt;Ying-Cong Chen, HKUST(GZ)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
    <item>
      <title>Our Paper “Ref-NeuS” Nominated for Best Paper at ICCV 2023</title>
      <link>http://localhost:1313/post/2023-ref-neus/</link>
      <pubDate>Wed, 04 Oct 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/2023-ref-neus/</guid>
      <description>&lt;p&gt;I am thrilled to share the exciting news that our paper, titled &amp;ldquo;Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection,&amp;rdquo; has been nominated for the Best Paper Award at the prestigious ICCV 2023 conference. This nomination is a testament to the hard work and innovative research conducted by our team. Below is a brief overview of our paper and its contributions.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;h3 id=&#34;abstract&#34;&gt;Abstract&lt;/h3&gt;
&lt;p&gt;Neural implicit surface learning has shown significant progress in multi-view 3D reconstruction, where an object is represented by multilayer perceptrons providing continuous implicit surface representation and view-dependent radiance. However, existing methods struggle to accurately reconstruct reflective surfaces, leading to severe ambiguity. Our proposed model, Ref-NeuS, addresses this issue by attenuating the importance of reflective surfaces through a novel approach.&lt;/p&gt;
&lt;h3 id=&#34;introduction&#34;&gt;Introduction&lt;/h3&gt;
&lt;p&gt;3D reconstruction is essential in computer vision, serving as the foundation for fields like computer-aided design, animation, and virtual reality. Image-based 3D reconstruction, which recovers 3D structures from 2D images, is particularly challenging. Traditional methods often require cumbersome multi-step pipelines. Our approach leverages neural implicit surface learning for end-to-end and unsupervised training, achieving high-quality reconstruction even on reflective surfaces.&lt;/p&gt;
&lt;h2 id=&#34;key-contributions&#34;&gt;Key Contributions&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Reflection-Aware Photometric Loss:&lt;/strong&gt; We introduce a reflection-aware photometric loss that reduces the influence of reflective surfaces. This is achieved by modeling the rendered color as a Gaussian distribution, where the reflection score represents the variance.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Reflection Score Estimation:&lt;/strong&gt; We utilize an anomaly detector to estimate an explicit reflection score with multi-view context guidance to localize reflective surfaces accurately.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;High-Quality Surface Reconstruction:&lt;/strong&gt; Our model significantly outperforms state-of-the-art methods in reconstructing reflective surfaces, providing more accurate surface geometry, normals, and rendering realism.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;results&#34;&gt;Results&lt;/h3&gt;
&lt;p&gt;Our extensive experiments demonstrate that Ref-NeuS achieves high-quality surface reconstruction on reflective surfaces, outperforming other methods by a large margin. The model is also competitive on general surfaces, showing its robustness and versatility.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Being nominated for the Best Paper Award at ICCV 2023 is a great honor for our team. It acknowledges our efforts to advance the field of 3D reconstruction with innovative solutions that address long-standing challenges. We look forward to presenting our work at the conference and engaging with the community to further this exciting research.&lt;/p&gt;
&lt;p&gt;For more details, you can access the full paper &lt;a href=&#34;lhttps://g3956.github.io/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Authors:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wenhang Ge, My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Tao Hu, Collaborator from SmartMore and CUHK&lt;/li&gt;
&lt;li&gt;Haoyu Zhao,  My student from HKUST(GZ)&lt;/li&gt;
&lt;li&gt;Shu Liu, Collaborator from SmartMore&lt;/li&gt;
&lt;li&gt;Ying-Cong Chen, HKUST(GZ)&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
</description>
    </item>
    
  </channel>
</rss>
